{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ2usiJNcEqL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUSqoWcUb04k"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "%cd Cap_Repo\n",
        "    \n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "image_size = 480\n",
        "caption_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
        "vqa_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth'    \n",
        "    \n",
        "model_caption = blip_decoder(pretrained=caption_model_url, image_size=image_size, vit='base')\n",
        "model_caption.eval()\n",
        "model_caption = model_caption.to(device)\n",
        "\n",
        "model_question = blip_vqa(pretrained=vqa_model_url, image_size=image_size, vit='base')\n",
        "model_question.eval()\n",
        "model_question = model_question.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP6f2L8scuTb"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMMyCxp4ct_l"
      },
      "outputs": [],
      "source": [
        "def load_image(image_size, device, img_url): #resize\n",
        "    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')   \n",
        "\n",
        "    w,h = raw_image.size\n",
        "    display(raw_image.resize((w//5,h//5)))\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ]) \n",
        "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
        "    return image\n",
        "\n",
        "def generate_caption(img_url):\n",
        "  image = load_image(image_size=image_size, device=device, img_url=imge_url)\n",
        "  with torch.no_grad():\n",
        "  caption = model_caption.generate(image, sample=False, num_beams=3, max_length=20, min_length=5) #input = image, sampling = true/false, beams = ?, max no of words in caption, min no of words in caption\n",
        "  print('caption: '+caption[0])\n",
        "\n",
        "def generate_answer(img_url, question): # eg. 'where is the woman sitting?'\n",
        "  image = load_image(image_size=image_size, device=device, img_url=imge_url)\n",
        "  with torch.no_grad():\n",
        "  answer = model_question(image_1, question, train=False, inference='generate') \n",
        "  print('answer: '+answer[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHbJO_19jIpG"
      },
      "outputs": [],
      "source": [
        "generate_caption('') #put url of image here\n",
        "generate_answer('', '') #put url of image here and question related to the picture"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
